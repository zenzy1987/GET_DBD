# -*- coding: utf-8 -*-
import os, re, time, random, json, argparse
from datetime import datetime, timezone
from urllib.parse import urljoin
from bs4 import BeautifulSoup

# Selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    TimeoutException, ElementClickInterceptedException, StaleElementReferenceException
)

# Google Sheets
import gspread
from google.oauth2.service_account import Credentials

PAGE_LOAD_TIMEOUT = 90
BASE = "https://datawarehouse.dbd.go.th"

# ========== Utils ==========
def canon_tax_id(x: str) -> str:
    """‡∏ó‡∏≥‡πÄ‡∏•‡∏Ç‡∏ú‡∏π‡πâ‡πÄ‡∏™‡∏µ‡∏¢‡∏†‡∏≤‡∏©‡∏µ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô: ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏° 0 ‡∏ã‡πâ‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö 13 ‡∏´‡∏•‡∏±‡∏Å"""
    t = re.sub(r"\D", "", str(x or ""))
    return t.zfill(13) if t else ""

# ========== Selenium helpers ==========
def build_driver():
    opts = Options()
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_argument("--window-size=1365,900")
    ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125 Safari/537.36"
    opts.add_argument(f"--user-agent={ua}")
    chrome_path = os.getenv("CHROME_PATH") or os.getenv("GOOGLE_CHROME_BIN")
    if chrome_path:
        opts.binary_location = chrome_path
    driver = webdriver.Chrome(options=opts)  # Selenium Manager ‡∏à‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ chromedriver ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏á
    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)
    return driver

def close_popup_if_any(driver):
    try:
        time.sleep(0.6)
        sels = ['#btnWarning', '.modal [data-bs-dismiss="modal"]', '.modal .btn-close', '.swal2-confirm']
        for sel in sels:
            for el in driver.find_elements(By.CSS_SELECTOR, sel):
                try:
                    if el.is_displayed() and el.is_enabled():
                        el.click(); time.sleep(0.3)
                except Exception:
                    pass
    except Exception:
        pass

def safe_open_link(driver, el):
    """‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏õ‡∏¥‡∏î‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÅ‡∏°‡πâ‡∏°‡∏µ overlay"""
    href = None
    try:
        href = el.get_attribute("href")
    except StaleElementReferenceException:
        pass
    if href:
        driver.get(urljoin(BASE, href))
        return True
    try:
        driver.execute_script("arguments[0].scrollIntoView({block:'center'});", el)
        time.sleep(0.15)
        el.click()
        return True
    except ElementClickInterceptedException:
        try:
            driver.execute_script("arguments[0].click();", el)
            return True
        except Exception:
            return False
    except Exception:
        return False

def go_home_and_search(driver, tax_id: str):
    driver.get(f"{BASE}/index")
    wait = WebDriverWait(driver, 40)
    close_popup_if_any(driver)
    sb = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, "input#key-word.form-control")))
    sb.clear(); sb.send_keys(tax_id); time.sleep(0.2); sb.send_keys(Keys.ENTER)

    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
    try:
        wait.until(EC.presence_of_element_located((By.XPATH, "//h4[contains(.,'‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•')]")))
        return
    except TimeoutException:
        pass

    # ‡∏´‡∏ô‡πâ‡∏≤ list ‚Üí ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    for label in ["‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î", "‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•"]:
        links = driver.find_elements(By.XPATH, f"//a[contains(.,'{label}')]")
        if links:
            close_popup_if_any(driver)
            if safe_open_link(driver, links[0]):
                return
            try:
                href = links[0].get_attribute("href")
                if href:
                    driver.get(urljoin(BASE, href))
                    return
            except Exception:
                pass

def wait_profile_loaded(driver):
    wait = WebDriverWait(driver, 40)
    wait.until(EC.presence_of_element_located((By.XPATH, "//h4[contains(.,'‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•')]")))
    time.sleep(0.6)

# ========== Parsing ==========
def extract_text_after_label(soup: BeautifulSoup, label: str) -> str:
    label_div = soup.find(lambda t: t.name == "div" and t.get_text(strip=True) == label)
    if not label_div: return ""
    parent = label_div.parent
    if parent:
        cols = parent.find_all("div", recursive=False)
        for i, c in enumerate(cols):
            if c is label_div and i + 1 < len(cols):
                return cols[i + 1].get_text(" ", strip=True)
    nxt = label_div.find_next_sibling("div")
    return nxt.get_text(" ", strip=True) if nxt else ""

def parse_profile_html(html: str):
    soup = BeautifulSoup(html, "html.parser")
    h3 = soup.find("h3"); h4 = soup.find("h4")
    name = re.sub(r"^‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•\s*:\s*", "", h3.get_text(" ", strip=True) if h3 else "") or "-"
    reg  = re.sub(r"^‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•:\s*", "", h4.get_text(" ", strip=True) if h4 else "") or "-"
    status_text = extract_text_after_label(soup, "‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•") or "-"
    reg_date    = extract_text_after_label(soup, "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á") or "-"
    capital     = extract_text_after_label(soup, "‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô") or "-"
    biz_group   = extract_text_after_label(soup, "‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à") or "-"
    biz_size    = extract_text_after_label(soup, "‡∏Ç‡∏ô‡∏≤‡∏î‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à") or "-"
    address     = extract_text_after_label(soup, "‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÅ‡∏´‡πà‡∏á‡πÉ‡∏´‡∏ç‡πà") or "-"

    directors = "-"
    h5_list = soup.find_all("h5", string=lambda s: s and "‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£" in s)
    if h5_list:
        body = h5_list[0].find_next("div", class_="card-body")
        if body:
            lis = [li.get_text(" ", strip=True).strip(" /") for li in body.find_all("li")]
            if lis: directors = ", ".join(lis)

    tsic1_code_name, tsic1_obj = "-", "-"
    h5_tsic1 = soup.find("h5", string=lambda s: s and "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ï‡∏≠‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô" in s)
    if h5_tsic1:
        body = h5_tsic1.find_next("div", class_="card-body")
        if body:
            lab = body.find(lambda t: t.name=="div" and t.get_text(strip=True)=="‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à")
            if lab and lab.find_next_sibling("div"): tsic1_code_name = lab.find_next_sibling("div").get_text(" ", strip=True)
            lab = body.find(lambda t: t.name=="div" and t.get_text(strip=True)=="‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå")
            if lab and lab.find_next_sibling("div"): tsic1_obj = lab.find_next_sibling("div").get_text(" ", strip=True)

    tsic2_code_name, tsic2_obj = "-", "-"
    h5_tsic2 = soup.find("h5", string=lambda s: s and "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏á‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏õ‡∏µ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î" in s)
    if h5_tsic2:
        body = h5_tsic2.find_next("div", class_="card-body")
        if body:
            lab = body.find(lambda t: t.name=="div" and t.get_text(strip=True)=="‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à")
            if lab and lab.find_next_sibling("div"): tsic2_code_name = lab.find_next_sibling("div").get_text(" ", strip=True)
            lab = body.find(lambda t: t.name=="div" and t.get_text(strip=True)=="‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå")
            if lab and lab.find_next_sibling("div"): tsic2_obj = lab.find_next_sibling("div").get_text(" ", strip=True)

    return {
        "‡∏ä‡∏∑‡πà‡∏≠": name, "‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô": reg, "‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞": status_text, "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô": reg_date,
        "‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô": capital, "‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à": biz_group, "‡∏Ç‡∏ô‡∏≤‡∏î‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à": biz_size,
        "‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏ç‡πà": address, "‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£": directors,
        "TSIC ‡∏ï‡∏≠‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô": tsic1_code_name, "‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ï‡∏≠‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô": tsic1_obj,
        "TSIC ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î": tsic2_code_name, "‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î": tsic2_obj,
    }

def scrape_one_id(driver, tax_id: str):
    for attempt in range(2):
        go_home_and_search(driver, tax_id)
        try:
            wait_profile_loaded(driver)
            break
        except TimeoutException:
            if attempt == 1: raise
            time.sleep(1.2)

    try:
        WebDriverWait(driver, 12).until(EC.presence_of_element_located((By.CSS_SELECTOR, ".tab1")))
        html = driver.find_element(By.CSS_SELECTOR, ".tab1").get_attribute("innerHTML") or driver.page_source
    except Exception:
        html = driver.page_source
    data = parse_profile_html(html)
    return None if not data.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô") or data["‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"] in ("-","") else data

# ========== Data / Sheets helpers ==========
HEADERS = [
    "tax_id","‡∏ä‡∏∑‡πà‡∏≠","‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô","‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞","‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô","‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô",
    "‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à","‡∏Ç‡∏ô‡∏≤‡∏î‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à","‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏ç‡πà","‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£",
    "TSIC ‡∏ï‡∏≠‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô","‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ï‡∏≠‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô","TSIC ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î","‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î",
    "fetched_at_utc"
]

def read_tax_ids(path: str):
    ids = []
    if path and os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.split("#", 1)[0].strip()
                if not line: 
                    continue
                m = re.search(r"\d{12,13}", line)  # ‡∏¢‡∏≠‡∏° 12‚Äì13 ‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏•‡πâ‡∏ß normalize ‡∏ï‡πà‡∏≠
                if m:
                    ids.append(canon_tax_id(m.group(0)))
    # unique (keep order)
    seen=set(); out=[]
    for x in ids:
        if x and x not in seen: seen.add(x); out.append(x)
    return out

def open_sheet():
    creds_json = os.getenv("GOOGLE_CREDENTIALS_JSON")
    if not creds_json:
        raise RuntimeError("GOOGLE_CREDENTIALS_JSON is empty.")
    info = json.loads(creds_json)
    scopes = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
    creds = Credentials.from_service_account_info(info, scopes=scopes)
    gc = gspread.authorize(creds)
    sheet_id = os.getenv("SHEET_ID")
    if not sheet_id:
        raise RuntimeError("SHEET_ID not set.")
    sh = gc.open_by_key(sheet_id)
    ws = sh.get_worksheet(0)  # gid=0
    first = ws.row_values(1)
    if first != HEADERS:
        ws.resize(1)
        ws.update("A1", [HEADERS])
    return ws

def existing_tax_ids_from_sheet(ws):
    col = ws.col_values(1)
    return {canon_tax_id(v) for v in col[1:] if v}

def existing_tax_ids_from_json(out_dir):
    if not os.path.isdir(out_dir): return set()
    return {canon_tax_id(fn[:-5]) for fn in os.listdir(out_dir) if fn.endswith(".json")}

def upsert_row(ws, row_dict):
    """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏ñ‡∏ß‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏î‡πâ‡∏ß‡∏¢ tax_id ‡πÅ‡∏ö‡∏ö canonical) ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô append ‡πÅ‡∏ñ‡∏ß‡πÉ‡∏´‡∏°‡πà"""
    tax_id = canon_tax_id(row_dict["tax_id"])
    col_vals = ws.col_values(1)  # ‡∏£‡∏ß‡∏° header
    row_index = None
    for i, v in enumerate(col_vals[1:], start=2):
        if canon_tax_id(v) == tax_id:
            row_index = i; break

    values = [row_dict.get(h, "") for h in HEADERS]
    # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô tax_id ‡πÄ‡∏õ‡πá‡∏ô "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°" ‡∏Å‡∏±‡∏ô‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏≤‡∏¢
    values[0] = f"'{tax_id}"

    if row_index:
        ws.update(f"A{row_index}", [values], value_input_option="RAW")
    else:
        ws.append_row(values, value_input_option="RAW")

# ========== Main ==========
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--tax-id", default=os.getenv("TAX_ID", "0135563016845"))
    ap.add_argument("--list-file", default="tax_ids.txt")
    ap.add_argument("--out-dir", default="data")
    ap.add_argument("--limit", type=int, default=40, help="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡πà‡∏≠‡∏£‡∏≠‡∏ö (0=‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)")
    ap.add_argument("--offset", type=int, default=0, help="‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏µ‡πà‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏£‡∏Å (‡∏õ‡∏Å‡∏ï‡∏¥‡∏õ‡∏•‡πà‡∏≠‡∏¢ 0 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ skip)")
    ap.add_argument("--skip-existing", choices=["none","sheet","json","both"], default="sheet",
                    help="‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô sheet/json")
    args = ap.parse_args()

    ids_all = read_tax_ids(args.list_file)
    if not ids_all:
        t = canon_tax_id(args.tax_id)
        if not re.fullmatch(r"\d{13}", t):
            print("‚ùå ‡πÉ‡∏™‡πà‡πÄ‡∏•‡∏Ç‡∏ú‡∏π‡πâ‡πÄ‡∏™‡∏µ‡∏¢‡∏†‡∏≤‡∏©‡∏µ 13 ‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"); return
        ids_all = [t]

    os.makedirs(args.out_dir, exist_ok=True)
    ws = open_sheet()

    # ‡∏Å‡∏£‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πà‡∏≠‡∏ô ‚Üí slice ‡∏ï‡∏≤‡∏° limit/offset
    done = set()
    if args.skip_existing in ("sheet","both"):
        done |= existing_tax_ids_from_sheet(ws)
    if args.skip_existing in ("json","both"):
        done |= existing_tax_ids_from_json(args.out_dir)

    remaining = [t for t in (canon_tax_id(x) for x in ids_all) if t not in done]
    start = max(args.offset, 0)
    end = (start + args.limit) if args.limit and args.limit > 0 else None
    ids = remaining[start:end]
    print(f"‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÉ‡∏ô‡∏Ñ‡∏¥‡∏ß {len(remaining)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‚Üí ‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥ {len(ids)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

    driver = build_driver()
    try:
        total = len(ids)
        for i, tax_id in enumerate(ids, start=1):
            print(f"\nüîé [{i}/{total}] ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏†‡∏≤‡∏©‡∏µ: {tax_id}")
            try:
                data = scrape_one_id(driver, tax_id)
            except Exception as e:
                print(f"‚ùå error: {e}")
                time.sleep(1.0)
                continue

            if not data:
                print("‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•/‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            else:
                canon = canon_tax_id(tax_id)
                row = {
                    "tax_id": canon,
                    **data,
                    "fetched_at_utc": datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
                }
                fp = os.path.join(args.out_dir, f"{canon}.json")
                with open(fp, "w", encoding="utf-8") as f:
                    json.dump(row, f, ensure_ascii=False, indent=2)
                print(f"üíæ saved: {fp}")
                upsert_row(ws, row)
                print("‚¨ÜÔ∏è  updated Google Sheets")

            time.sleep(random.uniform(1.0, 2.0))
    finally:
        try: driver.quit()
        except Exception: pass

if __name__ == "__main__":
    time.sleep(random.uniform(1.2, 2.5))
    main()